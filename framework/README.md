# Framework components
The Bagpipe Framework is a collection of dependent components, bootstrapped via the Bagpipe ApplicationService pattern:
   - [Configuration Module](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/utils/ConfigurationModule.scala): encapsulates TypeSafe-config-based application settings. See the resources/bagpipe.conf settings file for details. Most of the components depend on this module.
   - [Actor Module](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/utils/ActorModule.scala): encapsulates Akka ActorSystem resources. Most of the components depend on this module
   - [Persistence Module](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/utils/PersistenceModule.scala): encapsultaes a [Quill-IO](https://github.com/getquill/quill) based access to the Bagpipe data store.  The CRUD entity trait (see the Entity Traits section below) depends on this module's resources. By default, the Persistence module is configured to use PostgreSQL via [PostgreSQL-async](https://github.com/mauricio/postgresql-async) async-IO (non-JDBC implementation, uses a pool of Netty-managed connections and provides async/Future-based API).
   - [ServiceBus Module](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/utils/ServiceBusModule.scala): encapsulates the Akka EventBus-based internal service bus and provides APIs to publish and subscribe streaming data, as well as deploy custome stream processing logic. 
   - [Streaming Module](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/utils/StreamingModule.scala): encapsulates all core/built-in as well as custom/pluggable stream processing flow logic. This components deploys its flows on its Service Bus dependency
   - [HTTP RESTful](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/utils/HttpRestfulNetworkingModule.scala): is available in both Http Server and Http Client modules. Encapsulates a set of JSON-based RESTful APIs deployed using Akka-HTTP reactive streaming-IO technology. The REST entity trait (see the Entity Traits section below) provides the API definitions for this module.
   - [TCP](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/utils/TcpNetworkingModule.scala): is available in both Server and Client modules. Encapsulates a set of duplex Protobuf-binary-encoded streams, deployed using Akka-TCP reactive streaming-IO technology. The IO entity trait (see the Entity Traits section below) provides the streaming flow definition for this module.
   - [Messaging](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/utils/MessagingModule.scala): is available as two module vesions, which define AMQP messaging peers connected to a different set of Exchange/Queue resources. AMQP access is to a RabbitMQ broker is managed via [Reactive-Rabbit](https://github.com/ScalaConsultants/reactive-rabbit). The MQ entity trait (see the Entity Traits section below) provides the streaming flow definition for this module.
 
# Entity traits
All data in the Bagpipe is defined via a Protobuf-schema, with case classes generated by [ScalaPB](https://github.com/scalapb/ScalaPB). The [BaseEntity](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/utils/Entity.scala) generic trait defines a set of common get/set operations for any such entity - and the [EntityDefinition](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/utils/Entity.scala) defines all entity-level meta-data (for example, rich field definitions consumed by CRUD DB operations), including a comprehensive set of the following entity traits:
   - [CRUD](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/persistence/CRUD.scala): defines a set of common DB operations (simple CRUD, as well as polling and other common query APIs). 
   - [REST](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/rest/REST.scala): defines a set of HTTP-server Route (Akka-based Request->Response transformations) streaming flows. Note the Stream Route which makes the data available as a Server-Side Event (SSE) stream, implementing HTTP/1.1 compliant server push.
   - [IO](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/networking/IO.scala): defines outbound and inbound streaming flows that connect public endpoints with the Service Bus. All streaming-IO modules (HTTP, TCP) use this trait, with custom encoding/decoding added on top of it.
   - [MQ](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/messaging/MQ.scala): defines a simpler version of the streaming-IO flow connecting Bagpipe AMQP queues with the Service Bus. This trait depends on the IO trait.
The [entity definition Twirl template](https://github.com/atorson/bagpipe/blob/master/generator/src/main/twirl/net/andrewtorson/bagpipe/gen/templates/EntityTemplate.scala.txt) extends these traits with concrete Scala classes that are generated: even though most of the implementation code is available at the trait level, some code must reside in the concrete class (for example, custom field-level codecs used by Quill-IO - or Swagger API annotations that are required for compile-time reflection by the [Akka-Swagger](https://github.com/swagger-akka-http/swagger-akka-http) library). 
All entity definitions register themselves (at the construction time) and are accessible via the EntityDefinition object singleton and its apply() API. 

# Notable features and code patterns

The [ID](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/utils/ID.scala) feature provides a dynamic and flexible entity ID hierarchy (modeled as a Direct Acyclic Graph), as well as thread-safe ID-based registries that allow to add/remove ID-level code and execute onAdd/onRemove callbacks.  The ID feature is heavily used in nearly every Bagpipe component. IDs have a very small footprint (most of it is the Java-UUID, used in the equals() and hashCode() checks) and ID graph traversal and dependency resolution is very fast (this is particularly important for the high performance and scalability of the ServiceBus component).

The [StreamingPublisherActor](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/streaming/StreamingPublisherActor.scala) provides a core building block used in many Bagpipe Akka-Streams.  This component allows to connect data streams with non-stream sources while preserving backpressure (see the BackpressuredActorPublisherProxy that uses a blocking syncSend() method to acces its publisher actor that will throttle the data stream based on low/high-watermark backpressure strategy).  In particular, all ServiceBus sources are based on the publisher actor code.

The [EntityEventBus](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/eventbus/EventBus.scala) implementation uses hierarchical SubchannelClassification-based version of the Akka EventBus template. Hierarchy is dynamically defined via the ID functionality, which is specified via two graphs: a) message types (defines a verb hierarchy, mapping to actionable services) b) entity types/instances (allows to apply service to all entities/particular entity type/particular entity instance or a custom predicate atop these groups).  Most of the built-in Bagpipe flows apply themselves at the entity type level, because of the entity trait function defined at this level. 

The [StreamingFlowOps](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/streaming/StreamingFlowOps.scala) singleton provides a handful of custom, complex streaming flow constructs. In particular, it defines a generic adaptive workflow pattern (where new inputs may generate a custom stream of outputs based on the internal state that is also mutated as a result of input processing). The workflow pattern may be applied for many instances being run in parallel and independently: this functionality is provided via composite workflow pattern (which takes care of routing each instance's data to the correct flow, based on groupByKey() functionality).

The [CRUD](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/persistence/CRUD.scala) entity trait was inspired by Hibernate functionality (without inherinting Hibernate drawbacks) and allows: a) hydration/dehydration of dependent entities and easy traversal of the entity dependency graph b) easy definition and execution of complex, cascading transactions spanning multiple entity dependencies c) enforcement of foreign key relations at the application level. All these features were implemented using core 'oneOf' field type in the Protobuf standard. Basically, any foreign key relation is modeled as a oneOf field which may either have a dependency key or and entire dependent entity as its value.  Field-level entity meta-data (in particular, NestedFieldDefinition) is heavily used to implement these features.

Components that can be started and stopped are modeled via [ApplicationService](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/Boot.scala) pattern which allows custom service injections (Google Guice is ideal for that) and enforces ordered service orchestration/bootstrapping. 

# Building
The Bagpipe can be build using the SBT build tool from the root folder of the Bagpipe project:
       
       $ sbt package

# Running
By default, you should launch a PostgreSQL server, authorize connections to the Bagpipe system user (by default, it is the 'postgres' user that comes with PostgreSQL installation), configure 2 databases there: 'bagpipe' and 'bagpipetest' - and run the script [postgresql-schema.sql](https://github.com/atorson/bagpipe/blob/master/framework/src/main/resources/sql/postgresql-schema.sql) to initiate the schema. You should also launch a RabbitMQ server and authorize connections to the Bagpipe system user (by defualt, it is the 'guest' user that comes with RabbitMQ installation). Take a look at src/main/resources/bagpipe.conf file and change the DB and MQ configuration as you need. 

After that, launch the SBT build tool (at the 'frm' module level):

       $ sbt run

# Testing

To run all tests:

        $ sbt test

# Deployment

The Bagpipe framework module contains the [Bagpipe](https://github.com/atorson/bagpipe/blob/master/framework/src/main/scala/net/andrewtorson/bagpipe/Boot.scala) main class which can be launched. This is a simple Bagpipe application, containing the [Common](https://github.com/atorson/bagpipe/blob/master/framework/src/main/resources/proto/Common.proto) Bagpipe schema (Audit and Statistic entities) and core serices (persistence + broadcast routing services (anything coming over a public endpoint is broadcast into all other endpoints, after persistence)). The application launches Persistence module, HTTP and TCP server modules and MQ messaging module. 

There are two options for deployment of the core/framework Bagpipe application. 
The Docker packaging option uses the [Native Packaging SBT plugin](https://github.com/sbt/sbt-assembly)  which produces a Docker image of the project (that can be deployed and run by the Docker engine). This option can be invoked (at the 'frm' module level) and run as:
       
       $ sbt docker:publishLocal
       $ docker run -i -t  insert-your-Docker-image-name-here:inster-your-Docker-image-tag-here /bin/bash
Note that the resulting Docker image will not contain the PostgreSQL and RabbitMQ dependencies: these can be dockerized separately (say, downloaded from a Docker Hub) and then the resulting 3 Docker images can be served/orchestrated via a Docker Compose .yml file. Also, port forwarding need to be enabled in the Bagpipe docker for the DB and MQ ports to access these services running in separate containers.   

The Uber/Fat JAR packaging option uses [Assembly SBT plugin](https://github.com/sbt/sbt-assembly) which produces a fat jar (located in the /target/scala-x.xx folder), containing all of the project dependencies which can be launched using JVM). This option can be invoked and run as:

       $ sbt assembly
       $ java -cp insert-your-fat-jar-name-here -optionally-instert-your-full-main-class-name-here

# Using HTTP
The Bagpipe HTTP server can be accessed using any popular external HTTP clients. 
Curl usage:

  	create new DB record: curl -H "Content-Type: application/json" -X POST -d '{"namekey":"test1","family":"test","timeFrom":{   "millis":0},"timeTo":{"millis":0},"currentValue":0.0,"status": "Finalized"}' http://localhost:8080/Statistic

	poll existing DB records (created sinnce 1-Jan-1970): http://localhost:8080/Statistic/UpdatedSince?sinceTime=0

The test folder of the 'frm' module contains the [BagpipeTestBoot](https://github.com/atorson/bagpipe/blob/master/framework/src/test/scala/net/andrewtorson/bagpipe/BagpipeTestBoot.scala) test application which contains a Swagger GUI to test HTTP APIs. The Swagger browser app can be accessed via:

     	http://localhost:8080/swagger/index.html

# Using MQ
Open Rabbit MQ messaging console at http://localhost:15672, find the bagpipe.inbound and bagpipe.outbound queues, try to 
publish a message into the inbound.Statistic queue (copy/paste the JSON of the 'test1' statistic entity from the 'Using HTTP' section above) and check that it is persisted in the Bagpipe DB.

# Using DB
Use any SQL client to connect to connect to the PostgreSQL server via the port 5432 using the 'postgres' user credentials  (all configured in the bagpipe.conf file). Try creating the 'test1' statistic entity (either using HTTP or using MQ, as defined above) and use a SQL query against the 'bagpipe' data base 'statistic' table to see the 'test1' data record persisted 

# Using TCP
Use 'netcat' utility to connect to the Bagpipe TCP server via the 6544 port. Try creating the 'test1' statistic entity (either using HTTP or using MQ, as defined above) and check that netcat prints out the Protobuf-binary-encoded representation of this entity (it is partially human-readable):

	nc localhost 6544
